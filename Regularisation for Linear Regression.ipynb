{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation for Linear Regression\n",
    "\n",
    "Regularisation is a technique commonly used in Machine Learning to prevent overfitting. It consists on adding terms to the objective function such that the optimisation procedure avoids solutions that just learn the training data. Popular techniques for regularisation in Supervised Learning include Lasso Regression, Ridge Regression and the Elastic Net. \n",
    "\n",
    "In this Assignment, you will be looking at Ridge Regression and devising equations to optimise the objective function in Ridge Regression using two methods: a closed-form derivation and the update rules for stochastic gradient descent. You will then use those update rules for making predictions on a Air Quaility dataset.\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Let us start with a data set for training $\\mathcal{D} = \\{\\mathbf{y}, \\mathbf{X}\\}$, where the vector $\\mathbf{y}=[y_1, \\cdots, y_n]^{\\top}$ and $\\mathbf{X}$ is the design matrix from Lab 3, this is, \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} = \n",
    "                \\begin{bmatrix}\n",
    "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
    "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
    "                   \\vdots &  \\vdots\\\\\n",
    "                        1 & x_{n,1} & \\cdots & x_{n, D}\n",
    "                \\end{bmatrix}\n",
    "               = \n",
    "               \\begin{bmatrix}\n",
    "                      \\mathbf{x}_1^{\\top}\\\\\n",
    "                       \\mathbf{x}_2^{\\top}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\mathbf{x}_n^{\\top}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Our predictive model is going to be a linear model\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
    "\n",
    "The **objective function** we are going to use has the following form\n",
    "\n",
    "$$ J(\\mathbf{w}, \\alpha) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2 + \\frac{\\alpha}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "where $\\alpha>0$ is known as the *regularisation* parameter.\n",
    "\n",
    "The first term on the right-hand side (rhs) of the expression for $J(\\mathbf{w}, \\alpha)$ is very similar to the least-squares objective function we have seen before, for example in Lab 3. The only difference is on the term $\\frac{1}{n}$ that we use to normalise the objective with respect to the number of observations in the dataset. \n",
    "\n",
    "The first term on the rhs is what we call the \"fitting\" term whereas the second term in the expression is the regularisation term. Given $\\alpha$, the two terms in the expression have different purposes. The first term is looking for a value of $\\mathbf{w}$ that leads the squared-errors to zero. While doing this, $\\mathbf{w}$ can take any value and lead to a solution that it is only good for the training data but perhaps not for the test data. The second term is regularising the behavior of the first term by driving the $\\mathbf{w}$ towards zero. By doing this, it restricts the possible set of values that $\\mathbf{w}$ might take according to the first term. The value that we use for $\\alpha$ will allow a compromise between a value of $\\mathbf{w}$ that exactly fits the data (first term) or a value of $\\mathbf{w}$ that does not grow too much (second term).\n",
    "\n",
    "This type of regularisation has different names: ridge regression, Tikhonov regularisation or $\\ell_2$ norm regularisation. \n",
    "\n",
    "\n",
    "### Question 1: $J(\\mathbf{w}, \\alpha)$ in matrix form (2 marks)\n",
    "\n",
    "Write the expression for $J(\\mathbf{w}, \\alpha)$ in matrix form. Include ALL the steps necessary to reach the expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "We want to express \n",
    "\\begin{align*}\n",
    "    \\mathbf{J(w, \\alpha)} = \n",
    "      \\frac{1}{n} \n",
    "                \\begin{bmatrix}\n",
    "                        (y_1 -f(x_1))^2\\\\\n",
    "                       \\  (y_2 -f(x_2))^2\\\\\n",
    "                          \\vdots\\\\\n",
    "                        (y_n -f(x_n))^2\n",
    "                \\end{bmatrix}\n",
    "               +\n",
    "               \\frac{a}{2} \n",
    "               \\begin{bmatrix}\n",
    "                      \\ w_0^{2}\\\\\n",
    "                       \\ w_1^{2}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\ w_D^{2}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "We need to form a vector where the ith entry is f(x), this can be done with the design matrix.\n",
    "The fitting term becomes:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} (\\mathbf{y}-\\mathbf{X}\\mathbf{w})^T(\\mathbf{y}- \\mathbf{X}\\mathbf{w})\n",
    "$$\n",
    "\n",
    "The regularisation term is easily expressed as an inner product \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{a}{2} \\mathbf{w}^T \\mathbf{w}\n",
    "$$\n",
    "\n",
    "The final expression is therefore\n",
    "\n",
    "$$\n",
    "\\mathbf{J(w, \\alpha)} = \\frac{1}{n} (\\mathbf{y}-\\mathbf{X}\\mathbf{w})^T(\\mathbf{y}- \\mathbf{X}\\mathbf{w}) + \\frac{a}{2} \\mathbf{w}^T \\mathbf{w}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the objective function with respect to $\\mathbf{w}$\n",
    "\n",
    "There are two ways we can optimise the objective function with respect to $\\mathbf{w}$. The first one leads to a closed form expression for $\\mathbf{w}$ and the second one using an iterative optimisation procedure that updates the value of $\\mathbf{w}$ at each iteration by using the gradient of the objective function with respect to $\\mathbf{w}$,\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}},\n",
    "$$\n",
    "where $\\eta$ is the *learning rate* parameter and $\\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}}$ is the gradient of the objective function.\n",
    "\n",
    "### Question 2: Derivative of $J(\\mathbf{w}, \\alpha)$ wrt $\\mathbf{w}$ (2 marks)\n",
    "\n",
    "Find the closed-form expression for $\\mathbf{w}$ by taking the derivative of $J(\\mathbf{w}, \\alpha)$ with respect to \n",
    "$\\mathbf{w}$, equating to zero and solving for $\\mathbf{w}$. Write the expression in matrix form. \n",
    "\n",
    "Also, write down the specific update rule for $\\mathbf{w}_{\\text{new}}$ by using the equation above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 Answer\n",
    "$$\n",
    "\\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}} = \\frac{2}{n}(- \\mathbf{X}^T \\mathbf{y} + \\mathbf{X}^T \\mathbf{X} \\mathbf{w}) + \\alpha \\mathbf{w}      \n",
    "$$\n",
    "\n",
    "\n",
    "Setting the derivative  = 0 and solving for $\\mathbf{w}$\n",
    "\n",
    " \n",
    "$$ \\mathbf{w} = 2 \\frac{\\mathbf{X}^T \\mathbf{y}}{2\\mathbf{X}^T\\mathbf{X}+n\\alpha\\mathbf{I_{D+1}}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\big(  \\frac{2}{n}(- \\mathbf{X}^T \\mathbf{y} + \\mathbf{X}^T \\mathbf{X} \\mathbf{w}_{\\text{old}}) + \\alpha \\mathbf{w}_{\\text{old}} \\big)      \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ridge regression to predict air quality\n",
    "\n",
    "Our dataset comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We are going to use ridge regression for predicting air quality. The description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Air+Quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip -> .\\AirQualityUCI.zip\n",
      "[===========================   ]   1.328/1.472MB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [==============================]   1.472/1.472MB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "import pods\n",
    "pods.util.download_url('https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip')\n",
    "import zipfile\n",
    "zip = zipfile.ZipFile('./AirQualityUCI.zip', 'r')\n",
    "for name in zip.namelist():\n",
    "    zip.extract(name, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .csv version of the file has some typing issues, so we use the excel version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the rows in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>3.1</td>\n",
       "      <td>1287.75</td>\n",
       "      <td>-200</td>\n",
       "      <td>16.371624</td>\n",
       "      <td>1191.00</td>\n",
       "      <td>178.0</td>\n",
       "      <td>691.25</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1910.25</td>\n",
       "      <td>1192.00</td>\n",
       "      <td>19.775</td>\n",
       "      <td>44.300001</td>\n",
       "      <td>1.010436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>1537.50</td>\n",
       "      <td>-200</td>\n",
       "      <td>22.393233</td>\n",
       "      <td>1361.50</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>472.75</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1924.25</td>\n",
       "      <td>1792.75</td>\n",
       "      <td>25.650</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>1.768413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1317.00</td>\n",
       "      <td>-200</td>\n",
       "      <td>19.017971</td>\n",
       "      <td>1268.75</td>\n",
       "      <td>905.0</td>\n",
       "      <td>553.75</td>\n",
       "      <td>205.0</td>\n",
       "      <td>1445.75</td>\n",
       "      <td>1679.00</td>\n",
       "      <td>14.675</td>\n",
       "      <td>38.925000</td>\n",
       "      <td>0.646213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1181.00</td>\n",
       "      <td>-200</td>\n",
       "      <td>8.889612</td>\n",
       "      <td>935.00</td>\n",
       "      <td>158.0</td>\n",
       "      <td>701.25</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1564.75</td>\n",
       "      <td>960.25</td>\n",
       "      <td>24.875</td>\n",
       "      <td>56.025000</td>\n",
       "      <td>1.736685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>6.7</td>\n",
       "      <td>1571.00</td>\n",
       "      <td>-200</td>\n",
       "      <td>31.809165</td>\n",
       "      <td>1592.75</td>\n",
       "      <td>615.0</td>\n",
       "      <td>469.25</td>\n",
       "      <td>173.0</td>\n",
       "      <td>2408.50</td>\n",
       "      <td>1686.00</td>\n",
       "      <td>25.075</td>\n",
       "      <td>55.825000</td>\n",
       "      <td>1.751055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CO(GT)  PT08.S1(CO)  NMHC(GT)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
       "1534     3.1      1287.75      -200  16.371624        1191.00    178.0   \n",
       "5373  -200.0      1537.50      -200  22.393233        1361.50   -200.0   \n",
       "6043     4.2      1317.00      -200  19.017971        1268.75    905.0   \n",
       "5116     1.5      1181.00      -200   8.889612         935.00    158.0   \n",
       "4537     6.7      1571.00      -200  31.809165        1592.75    615.0   \n",
       "\n",
       "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)       T         RH  \\\n",
       "1534        691.25    123.0       1910.25      1192.00  19.775  44.300001   \n",
       "5373        472.75   -200.0       1924.25      1792.75  25.650  54.500000   \n",
       "6043        553.75    205.0       1445.75      1679.00  14.675  38.925000   \n",
       "5116        701.25     62.0       1564.75       960.25  24.875  56.025000   \n",
       "4537        469.25    173.0       2408.50      1686.00  25.075  55.825000   \n",
       "\n",
       "            AH  \n",
       "1534  1.010436  \n",
       "5373  1.768413  \n",
       "6043  0.646213  \n",
       "5116  1.736685  \n",
       "4537  1.751055  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable corresponds to the CO(GT) variable of the first column. The following columns correspond to the variables in the feature vectors, *e.g.*, PT08.S1(CO) is $x_1$ up until AH which is $x_D$. The original dataset also has a date and a time columns that we are not going to use in this assignment.\n",
    "\n",
    "Before designing our predictive model, we need to think about three stages: the preprocessing stage, the training stage and the validation stage. The three stages are interconnected and *it is important to remember that the testing data that we use for validation has to be set aside before preprocessing*. Any preprocessing that you do has to be done only on the training data and several key statistics need to be saved for the test stage.\n",
    "\n",
    "Separating the dataset into training and test before any preprocessing has happened help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "We are going to use *hold-out validation* for testing our predictive model so we need to separate the dataset into a training set and a test set.\n",
    "\n",
    "### Question 3: Splitting the dataset (1 mark)\n",
    "\n",
    "Split the dataset into a training set and a test set. The training set should have 70% of the total observations and the test set, the 30%. For making the random selection make sure that you use a random seed that corresponds to the last five digits of your student UCard. Make sure that you comment your code.\n",
    "\n",
    "#### Question 3 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.00</td>\n",
       "      <td>-200</td>\n",
       "      <td>-200.000000</td>\n",
       "      <td>-200.00</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.00</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.00</td>\n",
       "      <td>-200.00</td>\n",
       "      <td>-200.000000</td>\n",
       "      <td>-200.000000</td>\n",
       "      <td>-200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>1059.25</td>\n",
       "      <td>-200</td>\n",
       "      <td>8.350804</td>\n",
       "      <td>913.50</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>696.75</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1616.50</td>\n",
       "      <td>895.00</td>\n",
       "      <td>26.025000</td>\n",
       "      <td>47.650000</td>\n",
       "      <td>1.580582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>1092.75</td>\n",
       "      <td>-200</td>\n",
       "      <td>6.557207</td>\n",
       "      <td>837.25</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>829.50</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1479.75</td>\n",
       "      <td>1046.75</td>\n",
       "      <td>22.050000</td>\n",
       "      <td>55.349999</td>\n",
       "      <td>1.449521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1170.75</td>\n",
       "      <td>-200</td>\n",
       "      <td>13.137205</td>\n",
       "      <td>1088.25</td>\n",
       "      <td>168.0</td>\n",
       "      <td>821.75</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1889.25</td>\n",
       "      <td>1132.25</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>65.150002</td>\n",
       "      <td>1.525104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1024.25</td>\n",
       "      <td>91</td>\n",
       "      <td>4.682892</td>\n",
       "      <td>747.25</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1136.00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1382.00</td>\n",
       "      <td>897.25</td>\n",
       "      <td>9.650000</td>\n",
       "      <td>58.599998</td>\n",
       "      <td>0.703472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>1080.75</td>\n",
       "      <td>-200</td>\n",
       "      <td>14.003886</td>\n",
       "      <td>1116.75</td>\n",
       "      <td>102.0</td>\n",
       "      <td>689.25</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1668.25</td>\n",
       "      <td>828.00</td>\n",
       "      <td>35.749999</td>\n",
       "      <td>19.775001</td>\n",
       "      <td>1.139876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>0.7</td>\n",
       "      <td>853.25</td>\n",
       "      <td>-200</td>\n",
       "      <td>2.589178</td>\n",
       "      <td>625.50</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1044.50</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1556.50</td>\n",
       "      <td>515.50</td>\n",
       "      <td>26.650001</td>\n",
       "      <td>61.049999</td>\n",
       "      <td>2.100543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>1722.75</td>\n",
       "      <td>-200</td>\n",
       "      <td>44.686104</td>\n",
       "      <td>1865.75</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>378.25</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>2611.50</td>\n",
       "      <td>2006.00</td>\n",
       "      <td>24.300000</td>\n",
       "      <td>49.599999</td>\n",
       "      <td>1.486031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>1454.75</td>\n",
       "      <td>-200</td>\n",
       "      <td>26.534749</td>\n",
       "      <td>1467.50</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>474.25</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1945.00</td>\n",
       "      <td>1745.75</td>\n",
       "      <td>19.425000</td>\n",
       "      <td>61.150000</td>\n",
       "      <td>1.365173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1003.25</td>\n",
       "      <td>-200</td>\n",
       "      <td>7.357560</td>\n",
       "      <td>872.25</td>\n",
       "      <td>79.0</td>\n",
       "      <td>987.50</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1439.00</td>\n",
       "      <td>851.00</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>0.730364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CO(GT)  PT08.S1(CO)  NMHC(GT)    C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
       "1829  -200.0      -200.00      -200 -200.000000        -200.00   -200.0   \n",
       "4108  -200.0      1059.25      -200    8.350804         913.50   -200.0   \n",
       "4454  -200.0      1092.75      -200    6.557207         837.25   -200.0   \n",
       "1997     2.2      1170.75      -200   13.137205        1088.25    168.0   \n",
       "422      1.3      1024.25        91    4.682892         747.25    105.0   \n",
       "3384  -200.0      1080.75      -200   14.003886        1116.75    102.0   \n",
       "3728     0.7       853.25      -200    2.589178         625.50     22.0   \n",
       "4993  -200.0      1722.75      -200   44.686104        1865.75   -200.0   \n",
       "5346  -200.0      1454.75      -200   26.534749        1467.50   -200.0   \n",
       "1637     1.3      1003.25      -200    7.357560         872.25     79.0   \n",
       "\n",
       "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)           T  \\\n",
       "1829       -200.00   -200.0       -200.00      -200.00 -200.000000   \n",
       "4108        696.75   -200.0       1616.50       895.00   26.025000   \n",
       "4454        829.50   -200.0       1479.75      1046.75   22.050000   \n",
       "1997        821.75     81.0       1889.25      1132.25   20.200000   \n",
       "422        1136.00     75.0       1382.00       897.25    9.650000   \n",
       "3384        689.25    127.0       1668.25       828.00   35.749999   \n",
       "3728       1044.50     33.0       1556.50       515.50   26.650001   \n",
       "4993        378.25   -200.0       2611.50      2006.00   24.300000   \n",
       "5346        474.25   -200.0       1945.00      1745.75   19.425000   \n",
       "1637        987.50     80.0       1439.00       851.00   20.200000   \n",
       "\n",
       "              RH          AH  \n",
       "1829 -200.000000 -200.000000  \n",
       "4108   47.650000    1.580582  \n",
       "4454   55.349999    1.449521  \n",
       "1997   65.150002    1.525104  \n",
       "422    58.599998    0.703472  \n",
       "3384   19.775001    1.139876  \n",
       "3728   61.049999    2.100543  \n",
       "4993   49.599999    1.486031  \n",
       "5346   61.150000    1.365173  \n",
       "1637   31.200000    0.730364  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "air_quality = pd.read_excel('./AirQualityUCI.xlsx', usecols=range(2,15))\n",
    "air_quality =  pd.DataFrame(air_quality, columns = ['CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)','C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)','PT08.S3(NOx)', 'NO2(GT)','PT08.S4(NO2)','PT08.S5(O3)','T','RH','AH'])\n",
    "\n",
    "StudentID = 13983\n",
    "np.random.seed(StudentID) # setting the random seed\n",
    "Training_data_length = round(0.7 * air_quality.shape[0]) #returns 70% of the amount of records as a whole number \n",
    "Test_data_length = round(0.3 * air_quality.shape[0]) #Return 30% of the amount of records as a whole number\n",
    "\n",
    "\n",
    "indexes_users = np.random.permutation(Training_data_length) #return a permuted range\n",
    "my_batch_users_training = pd.DataFrame(np.zeros(shape= (Training_data_length,13)),columns = ['CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)','C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)','PT08.S3(NOx)', 'NO2(GT)','PT08.S4(NO2)','PT08.S5(O3)','T','RH','AH']) # make  a numpy array with length of training data inside a panda dataframe\n",
    "\n",
    "\n",
    "my_batch_users_training = air_quality.iloc[indexes_users,0:15]  #add air quality data to my training matrix\n",
    "   \n",
    "\n",
    "##process is repeated for test data\n",
    "indexes_users=np.random.permutation(Test_data_length)  \n",
    "   \n",
    "my_batch_users_testing = pd.DataFrame(np.zeros(shape= (Test_data_length,13)),columns =['CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)','C6H6', 'PT08.S2(NMHC)', 'NOx(GT)','PT08.S3(NOx)', 'NO2(GT)','PT08.S4(NO2)','PT08.S5(O3)','T','RH','AH'])\n",
    "my_batch_users_testing = air_quality.iloc[indexes_users,0:15]\n",
    "    \n",
    "my_batch_users_training.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "The dataset has missing values tagged with a -200 value. Before doing any work with the training data, we want to make sure that we deal properly with the missing values. \n",
    "\n",
    "### Question 4: Missing values (3 marks)\n",
    "\n",
    "Make some exploratory analysis on the number of missing values per column in the training data. \n",
    "\n",
    "* Remove the rows for which the target feature has missing values. We are doing supervised learning so we need all our data observations to have known target values.\n",
    "\n",
    "* Remove features with more than 20% of missing values. For all the other features with missing values, use the mean value of the non-missing values for imputation.\n",
    "\n",
    "#### Question 4 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>1.0</td>\n",
       "      <td>897.50</td>\n",
       "      <td>223.641573</td>\n",
       "      <td>4.572351</td>\n",
       "      <td>741.500000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>914.50</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>795.50</td>\n",
       "      <td>18.125000</td>\n",
       "      <td>65.175000</td>\n",
       "      <td>1.343018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0.7</td>\n",
       "      <td>899.50</td>\n",
       "      <td>223.641573</td>\n",
       "      <td>4.726454</td>\n",
       "      <td>749.500000</td>\n",
       "      <td>208.129114</td>\n",
       "      <td>932.00</td>\n",
       "      <td>100.13574</td>\n",
       "      <td>1494.250000</td>\n",
       "      <td>817.00</td>\n",
       "      <td>21.750000</td>\n",
       "      <td>46.625000</td>\n",
       "      <td>1.199131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>2.9</td>\n",
       "      <td>1251.50</td>\n",
       "      <td>223.641573</td>\n",
       "      <td>15.621359</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>556.50</td>\n",
       "      <td>107.00000</td>\n",
       "      <td>1937.500000</td>\n",
       "      <td>1352.25</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>47.424999</td>\n",
       "      <td>1.775284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5143</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1020.75</td>\n",
       "      <td>223.641573</td>\n",
       "      <td>5.982909</td>\n",
       "      <td>811.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>793.50</td>\n",
       "      <td>52.00000</td>\n",
       "      <td>1501.750000</td>\n",
       "      <td>836.75</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>67.924999</td>\n",
       "      <td>1.590064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1070.00</td>\n",
       "      <td>223.641573</td>\n",
       "      <td>6.844212</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>997.25</td>\n",
       "      <td>91.00000</td>\n",
       "      <td>1502.000000</td>\n",
       "      <td>778.25</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>0.907716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>1.0</td>\n",
       "      <td>934.50</td>\n",
       "      <td>223.641573</td>\n",
       "      <td>8.147875</td>\n",
       "      <td>905.250000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>899.00</td>\n",
       "      <td>69.00000</td>\n",
       "      <td>1675.250000</td>\n",
       "      <td>953.75</td>\n",
       "      <td>20.525000</td>\n",
       "      <td>54.750000</td>\n",
       "      <td>1.307293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6447</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1148.00</td>\n",
       "      <td>223.641573</td>\n",
       "      <td>9.599605</td>\n",
       "      <td>962.500000</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>679.50</td>\n",
       "      <td>102.00000</td>\n",
       "      <td>1394.500000</td>\n",
       "      <td>1016.50</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>83.899998</td>\n",
       "      <td>1.188997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1219.75</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>9.711401</td>\n",
       "      <td>966.750000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>843.50</td>\n",
       "      <td>117.00000</td>\n",
       "      <td>1545.000000</td>\n",
       "      <td>1218.50</td>\n",
       "      <td>11.825000</td>\n",
       "      <td>56.625000</td>\n",
       "      <td>0.783266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1.9</td>\n",
       "      <td>1188.50</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>6.714130</td>\n",
       "      <td>844.250000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>886.75</td>\n",
       "      <td>93.00000</td>\n",
       "      <td>1565.500000</td>\n",
       "      <td>1048.75</td>\n",
       "      <td>13.450000</td>\n",
       "      <td>69.600000</td>\n",
       "      <td>1.068779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>3.8</td>\n",
       "      <td>1224.00</td>\n",
       "      <td>223.641573</td>\n",
       "      <td>21.837693</td>\n",
       "      <td>1346.666667</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>602.00</td>\n",
       "      <td>176.00000</td>\n",
       "      <td>1669.666667</td>\n",
       "      <td>1901.00</td>\n",
       "      <td>26.433333</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>0.896908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CO(GT)  PT08.S1(CO)    NMHC(GT)   C6H6(GT)  PT08.S2(NMHC)     NOx(GT)  \\\n",
       "5167     1.0       897.50  223.641573   4.572351     741.500000   93.000000   \n",
       "2745     0.7       899.50  223.641573   4.726454     749.500000  208.129114   \n",
       "4023     2.9      1251.50  223.641573  15.621359    1168.000000  193.000000   \n",
       "5143     1.4      1020.75  223.641573   5.982909     811.000000  135.000000   \n",
       "1776     1.4      1070.00  223.641573   6.844212     850.000000   94.000000   \n",
       "2172     1.0       934.50  223.641573   8.147875     905.250000   88.000000   \n",
       "6447     2.5      1148.00  223.641573   9.599605     962.500000  451.000000   \n",
       "749      2.5      1219.75  235.000000   9.711401     966.750000  174.000000   \n",
       "871      1.9      1188.50  110.000000   6.714130     844.250000   96.000000   \n",
       "4842     3.8      1224.00  223.641573  21.837693    1346.666667  483.000000   \n",
       "\n",
       "      PT08.S3(NOx)    NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)          T  \\\n",
       "5167        914.50   50.00000   1345.000000       795.50  18.125000   \n",
       "2745        932.00  100.13574   1494.250000       817.00  21.750000   \n",
       "4023        556.50  107.00000   1937.500000      1352.25  28.100000   \n",
       "5143        793.50   52.00000   1501.750000       836.75  20.200000   \n",
       "1776        997.25   91.00000   1502.000000       778.25  24.750000   \n",
       "2172        899.00   69.00000   1675.250000       953.75  20.525000   \n",
       "6447        679.50  102.00000   1394.500000      1016.50  12.200000   \n",
       "749         843.50  117.00000   1545.000000      1218.50  11.825000   \n",
       "871         886.75   93.00000   1565.500000      1048.75  13.450000   \n",
       "4842        602.00  176.00000   1669.666667      1901.00  26.433333   \n",
       "\n",
       "             RH        AH  \n",
       "5167  65.175000  1.343018  \n",
       "2745  46.625000  1.199131  \n",
       "4023  47.424999  1.775284  \n",
       "5143  67.924999  1.590064  \n",
       "1776  29.500000  0.907716  \n",
       "2172  54.750000  1.307293  \n",
       "6447  83.899998  1.188997  \n",
       "749   56.625000  0.783266  \n",
       "871   69.600000  1.068779  \n",
       "4842  26.400000  0.896908  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_batch_users_training = my_batch_users_training[my_batch_users_training['CO(GT)'] != -200] # delete any missing value in first col\n",
    "missing_value_cap = (0.2 * my_batch_users_training.shape[0])\n",
    "\n",
    "for column in my_batch_users_training: # iterate through each row by column\n",
    "    if np.count_nonzero(column == -200) > missing_value_cap: #if the value is -200 \n",
    "        del my_batch_users_training[column] #remove feature\n",
    "        \n",
    "mean = [] #empty list to store mean values\n",
    "for column in my_batch_users_training:\n",
    "    mean.append(float(my_batch_users_training.loc[my_batch_users_training[column] != -200,[column]].mean())) #take the mean for each column\n",
    "i = 0  \n",
    "for column in my_batch_users_training:\n",
    "    my_batch_users_training.loc[my_batch_users_training[column] == -200, column] = mean[i] #if =-200 replace it with the mean\n",
    "    i += 1\n",
    "my_batch_users_training.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Normalising the training data (2 marks)\n",
    "\n",
    "Now that you have removed the missing data, we need to normalise the input vectors. \n",
    "\n",
    "* Explain in a sentence why do you need to normalise the input features for this dataset.\n",
    "\n",
    "* Normalise the training data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. Keep the mean values and standard deviations, you will need them at test time.\n",
    "\n",
    "#### Question 5 Answer\n",
    "\n",
    "\n",
    "Normalisation is required as the features have different ranges. \n",
    "One feature may influence the result more due to its larger value. It also means that the data will follow a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>-0.094230</td>\n",
       "      <td>0.497228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133781</td>\n",
       "      <td>0.285086</td>\n",
       "      <td>-0.464361</td>\n",
       "      <td>-0.230115</td>\n",
       "      <td>0.200009</td>\n",
       "      <td>0.356139</td>\n",
       "      <td>-0.376922</td>\n",
       "      <td>1.144695</td>\n",
       "      <td>-1.391498</td>\n",
       "      <td>-0.462083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545</th>\n",
       "      <td>0.114360</td>\n",
       "      <td>0.443845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164196</td>\n",
       "      <td>-0.025167</td>\n",
       "      <td>0.825396</td>\n",
       "      <td>-0.604012</td>\n",
       "      <td>-0.130616</td>\n",
       "      <td>-0.747518</td>\n",
       "      <td>0.404184</td>\n",
       "      <td>-0.927050</td>\n",
       "      <td>0.913949</td>\n",
       "      <td>-0.349448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6164</th>\n",
       "      <td>1.018248</td>\n",
       "      <td>1.182115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.873683</td>\n",
       "      <td>1.744989</td>\n",
       "      <td>3.969845</td>\n",
       "      <td>-1.443797</td>\n",
       "      <td>3.480829</td>\n",
       "      <td>-0.155665</td>\n",
       "      <td>2.264995</td>\n",
       "      <td>-1.751851</td>\n",
       "      <td>0.101787</td>\n",
       "      <td>-1.764017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>0.462009</td>\n",
       "      <td>0.218957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453592</td>\n",
       "      <td>0.591533</td>\n",
       "      <td>0.862703</td>\n",
       "      <td>-0.913615</td>\n",
       "      <td>0.327173</td>\n",
       "      <td>0.309302</td>\n",
       "      <td>0.350133</td>\n",
       "      <td>-0.245127</td>\n",
       "      <td>1.279710</td>\n",
       "      <td>1.185725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.511409</td>\n",
       "      <td>-0.082030</td>\n",
       "      <td>-2.219826</td>\n",
       "      <td>-0.879959</td>\n",
       "      <td>-0.934990</td>\n",
       "      <td>-0.528316</td>\n",
       "      <td>2.110205</td>\n",
       "      <td>0.098279</td>\n",
       "      <td>-0.761143</td>\n",
       "      <td>-0.727595</td>\n",
       "      <td>-1.946686</td>\n",
       "      <td>1.014749</td>\n",
       "      <td>-1.470341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>0.531539</td>\n",
       "      <td>-0.026376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.064799</td>\n",
       "      <td>0.081423</td>\n",
       "      <td>0.452326</td>\n",
       "      <td>-0.464542</td>\n",
       "      <td>0.225442</td>\n",
       "      <td>-0.110531</td>\n",
       "      <td>-0.291890</td>\n",
       "      <td>-0.754945</td>\n",
       "      <td>1.626751</td>\n",
       "      <td>0.524467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6206</th>\n",
       "      <td>1.157308</td>\n",
       "      <td>1.388831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018389</td>\n",
       "      <td>1.083560</td>\n",
       "      <td>2.712065</td>\n",
       "      <td>-1.273664</td>\n",
       "      <td>0.352606</td>\n",
       "      <td>0.202002</td>\n",
       "      <td>1.199791</td>\n",
       "      <td>-1.518049</td>\n",
       "      <td>1.901792</td>\n",
       "      <td>-0.503069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.971992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477386</td>\n",
       "      <td>0.613422</td>\n",
       "      <td>-0.368429</td>\n",
       "      <td>-0.622806</td>\n",
       "      <td>-0.156048</td>\n",
       "      <td>0.890084</td>\n",
       "      <td>0.686964</td>\n",
       "      <td>-0.274353</td>\n",
       "      <td>0.049946</td>\n",
       "      <td>-0.219764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>-0.859058</td>\n",
       "      <td>-1.101979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.870020</td>\n",
       "      <td>-0.919762</td>\n",
       "      <td>-0.528316</td>\n",
       "      <td>0.180381</td>\n",
       "      <td>-1.402251</td>\n",
       "      <td>-0.971485</td>\n",
       "      <td>-0.669589</td>\n",
       "      <td>-0.862105</td>\n",
       "      <td>1.799552</td>\n",
       "      <td>0.474661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>1.713547</td>\n",
       "      <td>1.744336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.692741</td>\n",
       "      <td>1.611752</td>\n",
       "      <td>2.184437</td>\n",
       "      <td>-1.532820</td>\n",
       "      <td>1.268183</td>\n",
       "      <td>1.052737</td>\n",
       "      <td>1.801605</td>\n",
       "      <td>-0.313320</td>\n",
       "      <td>0.811709</td>\n",
       "      <td>0.542272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CO(GT)  PT08.S1(CO)  NMHC(GT)  C6H6(GT)  PT08.S2(NMHC)   NOx(GT)  \\\n",
       "1698 -0.094230     0.497228  0.000000  0.133781       0.285086 -0.464361   \n",
       "6545  0.114360     0.443845  0.000000 -0.164196      -0.025167  0.825396   \n",
       "6164  1.018248     1.182115  0.000000  1.873683       1.744989  3.969845   \n",
       "5541  0.462009     0.218957  0.000000  0.453592       0.591533  0.862703   \n",
       "37   -0.511409    -0.082030 -2.219826 -0.879959      -0.934990 -0.528316   \n",
       "4753  0.531539    -0.026376  0.000000 -0.064799       0.081423  0.452326   \n",
       "6206  1.157308     1.388831  0.000000  1.018389       1.083560  2.712065   \n",
       "1742  0.044830     0.971992  0.000000  0.477386       0.613422 -0.368429   \n",
       "5552 -0.859058    -1.101979  0.000000 -0.870020      -0.919762 -0.528316   \n",
       "6430  1.713547     1.744336  0.000000  1.692741       1.611752  2.184437   \n",
       "\n",
       "      PT08.S3(NOx)   NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)         T        RH  \\\n",
       "1698     -0.230115  0.200009      0.356139    -0.376922  1.144695 -1.391498   \n",
       "6545     -0.604012 -0.130616     -0.747518     0.404184 -0.927050  0.913949   \n",
       "6164     -1.443797  3.480829     -0.155665     2.264995 -1.751851  0.101787   \n",
       "5541     -0.913615  0.327173      0.309302     0.350133 -0.245127  1.279710   \n",
       "37        2.110205  0.098279     -0.761143    -0.727595 -1.946686  1.014749   \n",
       "4753     -0.464542  0.225442     -0.110531    -0.291890 -0.754945  1.626751   \n",
       "6206     -1.273664  0.352606      0.202002     1.199791 -1.518049  1.901792   \n",
       "1742     -0.622806 -0.156048      0.890084     0.686964 -0.274353  0.049946   \n",
       "5552      0.180381 -1.402251     -0.971485    -0.669589 -0.862105  1.799552   \n",
       "6430     -1.532820  1.268183      1.052737     1.801605 -0.313320  0.811709   \n",
       "\n",
       "            AH  \n",
       "1698 -0.462083  \n",
       "6545 -0.349448  \n",
       "6164 -1.764017  \n",
       "5541  1.185725  \n",
       "37   -1.470341  \n",
       "4753  0.524467  \n",
       "6206 -0.503069  \n",
       "1742 -0.219764  \n",
       "5552  0.474661  \n",
       "6430  0.542272  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_deviation =[]\n",
    "#means were calculated in previous question and will remain the same\n",
    "\n",
    "for column in my_batch_users_training:\n",
    "    standard_deviation.append(my_batch_users_training[column].std()) #calculate standard deviation for each column\n",
    "    \n",
    "i =0\n",
    "for column in my_batch_users_training: # loops through each column and replaces each value with normalised value\n",
    "    my_batch_users_training.loc[:,column] = (my_batch_users_training.loc[:,column]-mean[i])/standard_deviation[i] \n",
    "    i += 1\n",
    "\n",
    "my_batch_users_training.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation stages\n",
    "\n",
    "We have now curated our training data by removing data observations and features with a large amount of missing values. We have also normalised the feature vectors. We are now in a good position to work on developing the prediction model and validating it. We will use both the closed form expression for $\\mathbf{w}$ and gradient descent for iterative optimisation. \n",
    "\n",
    "We first organise the dataframe into the vector of targets $\\mathbf{y}$ and the design matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to get y and X\n",
    "y = my_batch_users_training.copy(deep = True )\n",
    "del y['CO(GT)']\n",
    "X = np.hstack((np.ones_like(my_batch_users_training['CO(GT)']), my_batch_users_training['CO(GT)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: training with closed form expression for $\\mathbf{w}$ (3 marks)\n",
    "\n",
    "To find the optimal value of $\\mathbf{w}$ using the closed form expression that you derived before, we need to know the value of the regularisation parameter $\\alpha$ in advance. We will determine the value by using part of the training data for finding the parameters $\\mathbf{w}$ and another part of the training data to choose the best $\\alpha$ from a set of predefined values.\n",
    "\n",
    "* Use `np.log(start, stop, num)` to create a set of values for $\\alpha$ in log scale. Use the following parameters `start=-3`, `stop=2` and `num=20`. \n",
    "\n",
    "* Randomly split the training data into what is properly called the training set and the validation set. As before, make sure that you use a random seed that corresponds to the last five digits of your student UCard. Use 70% of the data for the training set and 30% of the data for the validation set.\n",
    "\n",
    "* For each value that you have for $\\alpha$ from the previous step, use the training set to compute $\\mathbf{w}$ and then measure the mean-squared error (MSE) over the validation data. After this, you will have `num=20` MSE values. Choose the value of $\\alpha$ that leads to the lower MSE and save it. You will use it at the test stage.\n",
    "\n",
    "* What was the best value of $\\alpha$? Is there any explanation for that?\n",
    "\n",
    "#### Question 6 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of alpha is 0.001\n"
     ]
    }
   ],
   "source": [
    "a = np.logspace(start=-3,stop=2,num=20)\n",
    "#the seed has been set already\n",
    "\n",
    "#training set\n",
    "indexes_users = np.random.permutation(round(0.7 * my_batch_users_training.shape[0])) #return a permuted range\n",
    "training_set = my_batch_users_training.iloc[indexes_users,0:13]  #add training data to my training set\n",
    "xTrainingSet = training_set.iloc[:,1:]\n",
    "ytrainingSet = training_set.iloc[:,0]\n",
    "#xTrainingSet.insert(0,'ones', [1] * xTrainingSet.shape[0]) #make indexes match\n",
    "\n",
    "#validation set\n",
    "indexes_users = np.random.permutation(round(0.3 * my_batch_users_training.shape[0])) #return a permuted range\n",
    "validation_set = my_batch_users_training.iloc[indexes_users,0:13]  #add training data to my training set\n",
    "XValSet = validation_set.iloc[:,1:]\n",
    "yValSet = validation_set.iloc[:,0]\n",
    "#XValSet.insert(0,'ones',[1] * XValSet.shape[0]) #make indexes match\n",
    "\n",
    "w = np.zeros(shape=(13, 1))\n",
    "I = np.identity(xTrainingSet.shape[1])\n",
    "i = 0\n",
    "\n",
    "\n",
    "mse = [None] * 20\n",
    "\n",
    "for j in range(20):\n",
    "    w = np.linalg.solve(2 * np.dot(xTrainingSet.T,xTrainingSet) + ytrainingSet.size * a[j] *I,np.dot(xTrainingSet.T,ytrainingSet))\n",
    "    mse[j] = (1/yValSet.size) * np.dot((np.dot(XValSet,w) - yValSet).T,np.dot(XValSet,w)- yValSet)                            \n",
    "\n",
    "\n",
    "alpha = a[mse.index(min(mse))]\n",
    "print(\"Best value of alpha is \" + str(alpha))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are trying to minimise the objective function:\n",
    "$$\n",
    "\\mathbf{J(w, \\alpha)} = \\frac{1}{n} (\\mathbf{y}-\\mathbf{X}\\mathbf{w})^T(\\mathbf{y}- \\mathbf{X}\\mathbf{w}) + \\frac{a}{2} \\mathbf{w}^T \\mathbf{w}\n",
    "$$\n",
    "$ \\mathbf{w}^T \\mathbf{w}$ is always positive. $\\mathbf{J(w, \\alpha)} $ decreases with a smaller $ \\alpha $.Therefore,the best value for $\\alpha$ is going to the smallest value which is 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: validation with the closed form expression for $\\mathbf{w}$ (2 marks)\n",
    "\n",
    "We are going to deal now with the test data to perform the validation of the model. Remember that the test data might also contain missing values in the target variable and in the input features.\n",
    "\n",
    "* Remove the rows of the test data for which the labels have missing values. \n",
    "* If you remove any feature at the training stage, you also need to remove the same features from the test stage.\n",
    "* Replace the missing values on each feature variables with the mean value you computed in the training data.\n",
    "* Normalise the test data using the means and standard deviations computed from the training data\n",
    "* Compute again $\\mathbf{w}$ for the value of $\\alpha$ that best performed on the validation set using ALL the training data (not all the training set).\n",
    "* Report the MSE on the preprocessed test data and an histogram with the absolute error.\n",
    "* Does the regularisation have any effect on the model? Explain your answer.\n",
    "\n",
    "#### Question 7 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 0.22749735213149352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Absolute Error')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "#1\n",
    "my_batch_users_testing = my_batch_users_testing[my_batch_users_testing['CO(GT)'] != -200] # delete any missing value in first col\n",
    "\n",
    "#2\n",
    "#no features were removed from training stage\n",
    "\n",
    "#3&4\n",
    "i =0\n",
    "for column in my_batch_users_testing:\n",
    "    my_batch_users_testing.loc[my_batch_users_testing[column] == -200, column] = mean[i] #if =-200 replace it with the mean\n",
    "    my_batch_users_testing.loc[:,column] = (my_batch_users_testing.loc[:,column]-mean[i])/standard_deviation[i]\n",
    "    i += 1\n",
    "\n",
    "XTestSet = my_batch_users_testing.iloc[:,1:]\n",
    "yTestSet = my_batch_users_testing.iloc[:,0]\n",
    "#XTestSet.insert(0,'ones',[1] * XTestSet.shape[0]) #make indexes match\n",
    "\n",
    "XAlltrainingdata = air_quality.iloc[:,1:]\n",
    "YAlltrainingdata = air_quality.iloc[:,0]\n",
    "#XAlltrainingdata.insert(0,'ones',[1]* XAlltrainingdata.shape[0])\n",
    "\n",
    "w2 = np.zeros(shape=(13, 1))\n",
    "#5\n",
    "I = np.identity(xTrainingSet.shape[1]) #identity matrix has to be same size as no of columns \n",
    "w2 = np.linalg.solve(2 * np.dot(XTestSet.T,XTestSet) + yTestSet.size *alpha* I,np.dot(XTestSet.T,yTestSet))\n",
    "\n",
    "#6\n",
    "absolute_error = [None] * XTestSet.shape[0] #list to store the absolute error values\n",
    "mse2=  (1/XTestSet.size) * np.dot((np.dot(XTestSet,w2) - yTestSet).T,np.dot(XTestSet,w2)- yTestSet)\n",
    "j=0\n",
    "for j in range(yTestSet.shape[0]):\n",
    "    absolute_error = abs(yTestSet - np.dot(XTestSet,w2))\n",
    "print(\"MSE is \" + str(mse2))\n",
    "plt.hist(absolute_error,bins = 20) \n",
    "plt.title('Histogram to Show Absolute Error for the Validation Set')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Absolute Error')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularisation does have an effect on the model as it constrains the data and so minimises the adjusted loss function. This is desirable as it reduces the computational complexity and can stop overfitting of unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: training with gradient descent and validation (5 marks)\n",
    "\n",
    "\n",
    "Use gradient descent to iteratively compute the value of $\\mathbf{w}_{\\text{new}}$. Instead of using all the training set to compute the gradient, use a subset of $B$ datapoints in the training set. This is sometimes called minibatch gradient descent where $B$ is the size of the minibacth. When using gradient descent with minibatches, you need to find the best values for three parameters: $\\eta$, the learning rate, $B$, the number of datapoints in the minibatch and $\\alpha$, the regularisation parameter.\n",
    "\n",
    "* As you did on Question 6, create a grid of values for the parameters $\\alpha$ and $\\eta$ using `np.logspace` and a grid of values for $B$ using np.linspace. Because you need to find \n",
    " three parameters, start with `num=5` and see if you can increase it.\n",
    "\n",
    "* Use the same training set and validation set that you used in Question 6.\n",
    "\n",
    "* For each value that you have of $\\alpha$, $\\eta$ and $B$ from the previous step, use the training set to compute $\\mathbf{w}$ using minibatch gradient descent and then measure the MSE over the validation data. For the minibatch gradient descent choose to stop the iterative procedure after $500$ iterations.\n",
    "\n",
    "* Choose the values of $\\alpha$, $\\eta$ and $B$ that lead to the lower MSE and save them. You will use them at the test stage.\n",
    "\n",
    "*3 marks of out of the 5 marks*\n",
    "\n",
    "\n",
    "* Use the test set from Question 7 and provide the MSE obtained by having used minibatch training with the best values for $\\alpha$, $\\eta$ and $B$ over the WHOLE training data (not only the training set).\n",
    "\n",
    "* Compare the performance of the closed form solution and the minibatch solution. Are the performances similar? Are the parameters $\\mathbf{w}$ and $\\alpha$ similar in both approaches? Please comment on both questions.\n",
    "\n",
    "*2 marks of out of the 5 marks*\n",
    "\n",
    "#### Question 8 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSE is 1.0343938428424555\n",
      "Best value of alpha is 0.001\n",
      "Best value of n is 0.001\n",
      "Best value of b is 20.0\n",
      "MSE obtained using minibatch training over whole training data  0.7852528975914652\n"
     ]
    }
   ],
   "source": [
    "num1 = 5\n",
    "a2 = np.logspace(start = -3, stop = 2,num= num1)\n",
    "n  = np.logspace(start = -3,stop = 2,num= num1)\n",
    "b = np.linspace(start = 20 ,stop = 100,num = num1) # I choose 5 batches\n",
    "\n",
    "iterations = 500\n",
    "mse3 = [None] * num1\n",
    "new_w = np.zeros(shape=(13, 1)) #defined a new w \n",
    "j= 0\n",
    "for i in range(iterations):\n",
    "    for j in range(num1):\n",
    "        for k in range(int(b[j])):\n",
    "              #temp = training_set.sample(frac=1).reset_index(drop=True)# the training data for batch sampling\n",
    "            #temp_train= temp.iloc[0:int(b)]\n",
    "            new_w = - n[j] * np.linalg.solve(2 * np.dot(xTrainingSet.T,xTrainingSet) + ytrainingSet.size* a[j] * I,np.dot(xTrainingSet.T,ytrainingSet))\n",
    "            mse3[j] = (1/yValSet.size) * np.dot((np.dot(XValSet,new_w) - yValSet).T,np.dot(XValSet,new_w)- yValSet)          \n",
    "\n",
    "#Report Best Values\n",
    "best_mse3 = mse3[mse3.index(min(mse3))]\n",
    "best_alpha = a2[mse3.index(min(mse3))]        \n",
    "best_n = n[mse3.index(min(mse3))] # ideallly this should be fairly small but not so slow that the code takes long to run\n",
    "best_b = b[mse3.index(min(mse3))]\n",
    "\n",
    "print(\"Lowest MSE is \" + str(best_mse3))\n",
    "print(\"Best value of alpha is \" + str(best_alpha))\n",
    "print(\"Best value of n is \" + str(best_n))\n",
    "print(\"Best value of b is \" + str(best_b))\n",
    "\n",
    "for i in range(int(best_b)):\n",
    "    best_w = - best_n * np.linalg.solve(2 * np.dot(XTestSet.T,XTestSet) + yTestSet.size * best_alpha * I,np.dot(XTestSet.T,yTestSet))\n",
    "\n",
    "mse4 = (1/yTestSet.size) * np.dot((np.dot(XTestSet,best_w) - yTestSet).T,np.dot(XTestSet,best_w)- yTestSet)\n",
    "\n",
    "print(\"MSE obtained using minibatch training over whole training data \", mse4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances are not similiar, $\\alpha$ is the same in both cases and so the regularisation of the data set is similiar. The only difference is that the model is trained with less input data and thus means that the model does generalise the data as well as closed loop. \n",
    "\n",
    "The MSE using closed loop was 0.22749735213149352 compared to 0.7852528975914652 for minibatch descent. Closed loop fits the validation data better. The performance is around 4 times better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
